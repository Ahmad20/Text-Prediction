{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neural based"
      ],
      "metadata": {
        "id": "CCYh3a-xxB52"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "source : https://alvinntnu.github.io/python-notes/nlp/neural-language-model-primer.html"
      ],
      "metadata": {
        "id": "sdrFyl1WAXnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "4bLQ3VVfmhiZ",
        "outputId": "3d6174e5-ab4c-4ac3-b4b7-8690caff82c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.0+zzzcolab20220506162203)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.46.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.26.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.7)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
            "Installing collected packages: tf-estimator-nightly\n",
            "Successfully installed tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy import array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "# generate a sequence from a language model\n",
        "def generate_seq(model, tokenizer, max_length, seed_text, n_words):\n",
        "  in_text = seed_text\n",
        "  # generate a fixed number of words\n",
        "  for _ in range(n_words):\n",
        "    # encode the text as integer\n",
        "    encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "    # pre-pad sequences to a fixed length\n",
        "    encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
        "    # predict probabilities for each word\n",
        "    # yhat = model.predict_classes(encoded, verbose=0)\n",
        "    # yhat = model.predict(encoded)\n",
        "    # yhat = np.round(yhat).astype(int)\n",
        "    yhat = np.argmax(model.predict(encoded), axis=-1)\n",
        "    # map predicted word index to word\n",
        "    out_word = ''\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "      if index == yhat:\n",
        "        out_word = word\n",
        "        break\n",
        "    # append to input\n",
        "    in_text += ' ' + out_word\n",
        "  return in_text\n",
        "\n",
        "# source text\n",
        "# data = \"\"\" Jack and Jill went up the hill\\n\n",
        "# \t\tTo fetch a pail of water\\n\n",
        "# \t\tJack fell down and broke his crown\\n\n",
        "# \t\tAnd Jill came tumbling after\\n \"\"\"\n",
        "file = open('/clean.txt', 'r', encoding='utf-8')\n",
        "data = \"\"\n",
        "while True:\n",
        "    line = file.readline()\n",
        "    data += line\n",
        "    if not line:\n",
        "        break\n",
        "# integer encode sequences of words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "encoded = tokenizer.texts_to_sequences([data])[0]\n",
        "# retrieve vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "# encode 2 words -> 1 word\n",
        "sequences = list()\n",
        "for i in range(2, len(encoded)):\n",
        "\tsequence = encoded[i-2:i+1]\n",
        "\tsequences.append(sequence)\n",
        "print('Total Sequences: %d' % len(sequences))\n",
        "# pad sequences\n",
        "max_length = max([len(seq) for seq in sequences])\n",
        "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
        "print('Max Sequence Length: %d' % max_length)\n",
        "# split into input and output elements\n",
        "sequences = array(sequences)\n",
        "X, y = sequences[:,:-1],sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=max_length-1))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())\n",
        "# compile network\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit network\n",
        "model.fit(X, y, epochs=500, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "ksc3OGPAmcIn",
        "outputId": "e2a43bcb-dc3f-45ca-da57-7f7cc977ad7e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-7ed1e006684b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m#               Jack fell down and broke his crown\\n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m#               And Jill came tumbling after\\n \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/clean.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/clean.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model\n",
        "print(generate_seq(model, tokenizer, max_length-1, 'hung upon peg she took down', 6))\n",
        "print(generate_seq(model, tokenizer, max_length-1, 'time she said aloud', 3))\n",
        "print(generate_seq(model, tokenizer, max_length-1, 'began talking', 5))\n",
        "print(generate_seq(model, tokenizer, max_length-1, 'all dark', 5))"
      ],
      "metadata": {
        "id": "wXC1y9l-nDTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trigram Model"
      ],
      "metadata": {
        "id": "xbBZ7_0BxGLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "source : https://github.com/jadessechan/Text-Prediction"
      ],
      "metadata": {
        "id": "uA9tXvV5AeiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "import string\n",
        "import random\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.probability import ConditionalFreqDist\n",
        "\n",
        "\n",
        "def main():\n",
        "    file = open('/alice.txt', 'r', encoding='utf-8')\n",
        "    text = \"\"\n",
        "    while True:\n",
        "        line = file.readline()\n",
        "        text += line\n",
        "        if not line:\n",
        "            break\n",
        "\n",
        "    # pre-process text\n",
        "    print(\"Filtering...\")\n",
        "    words = filter(text)\n",
        "    print(\"Cleaning...\")\n",
        "    words = clean(words)\n",
        "    clean_text = ' '.join([str(item) for item in words])\n",
        "    # make language model\n",
        "    print(\"Making model...\")\n",
        "    model = n_gram_model(words)\n",
        "\n",
        "    print(\"Enter a phrase: \")\n",
        "    user_input = input()\n",
        "    predict(model, user_input)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    Normalize text, remove unnecessary characters, \n",
        "    perform regex parsing, and make lowercase\n",
        "\"\"\"\n",
        "def filter(text):\n",
        "    # normalize text\n",
        "    text = (unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore'))\n",
        "    # replace html chars with ' '\n",
        "    text = re.sub('<.*?>', ' ', text)\n",
        "    # remove punctuation\n",
        "    text = text.translate(str.maketrans(' ', ' ', string.punctuation))\n",
        "    # only alphabets and numerics\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "    # replace newline with space\n",
        "    text = re.sub(\"\\n\", \" \", text)\n",
        "    # lower case\n",
        "    text = text.lower()\n",
        "    # split and join the words\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text\n",
        "\n",
        "\"\"\"\n",
        "    Tokenize remaining words\n",
        "    and perform lemmatization\n",
        "\"\"\"\n",
        "def clean(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    wnl = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "    output = []\n",
        "    for words in tokens:\n",
        "        # lemmatize words\n",
        "        output.append(wnl.lemmatize(words))\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    Make a language model using a dictionary, trigrams, \n",
        "    and calculate word probabilities\n",
        "\"\"\"\n",
        "def n_gram_model(text):\n",
        "    trigrams = list(nltk.ngrams(text, 3, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n",
        "    # bigrams = list(nltk.ngrams(text, 2, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n",
        "\n",
        "# N-gram Statistics\n",
        "    # get freq dist of trigrams\n",
        "    # freq_tri = nltk.FreqDist(trigrams)\n",
        "    # freq_bi = nltk.FreqDist(bigrams)\n",
        "    # freq_tri.plot(30, cumulative=False)\n",
        "    # print(\"Most common trigrams: \", freq_tri.most_common(5))\n",
        "    # print(\"Most common bigrams: \", freq_bi.most_common(5))\n",
        "\n",
        "    # make conditional frequencies dictionary\n",
        "    cfdist = ConditionalFreqDist()\n",
        "    for w1, w2, w3 in trigrams:\n",
        "        cfdist[(w1, w2)][w3] += 1\n",
        "\n",
        "    # transform frequencies to probabilities\n",
        "    for w1_w2 in cfdist:\n",
        "        total_count = float(sum(cfdist[w1_w2].values()))\n",
        "        for w3 in cfdist[w1_w2]:\n",
        "            cfdist[w1_w2][w3] /= total_count\n",
        "\n",
        "    return cfdist\n",
        "\n",
        "\"\"\"\n",
        "    Generate predictions from the Conditional Frequency Distribution\n",
        "    dictionary (param: model), append weighted random choice to user's phrase,\n",
        "    allow option to generate more words following the prediction\n",
        "\"\"\"\n",
        "def predict(model, user_input):\n",
        "    user_input = filter(user_input)\n",
        "    user_input = user_input.split()\n",
        "\n",
        "    w1 = len(user_input) - 2\n",
        "    w2 = len(user_input)\n",
        "    prev_words = user_input[w1:w2]\n",
        "\n",
        "    # display prediction from highest to lowest maximum likelihood\n",
        "    prediction = sorted(dict(model[prev_words[0], prev_words[1]]), key=lambda x: dict(model[prev_words[0], prev_words[1]])[x], reverse=True)\n",
        "    print(\"Trigram model predictions: \", prediction)\n",
        "\n",
        "    word = []\n",
        "    weight = []\n",
        "    for key, prob in dict(model[prev_words[0], prev_words[1]]).items():\n",
        "        word.append(key)\n",
        "        weight.append(prob)\n",
        "    # pick from a weighted random probability of predictions\n",
        "    next_word = random.choices(word, weights=weight, k=1)\n",
        "    # add predicted word to user input\n",
        "    user_input.append(next_word[0])\n",
        "    print(' '.join(user_input))\n",
        "\n",
        "    ask = input(\"Do you want to generate another word? (type 'y' for yes or 'n' for no): \")\n",
        "    if ask.lower() == 'y':\n",
        "        predict(model, str(user_input))\n",
        "    elif ask.lower() == 'n':\n",
        "        print(\"done\")\n",
        "        \n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "id": "J_nf-CEJxFVt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "497338f01161d141240caa60efca5087225791fadc9b7c964beb65d863e046f0"
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit (windows store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "Tugas Ekperimen 1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}